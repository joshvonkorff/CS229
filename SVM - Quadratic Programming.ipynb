{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quadprog\n",
    "import qpsolvers\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program accepts X, a matrix whose rows are the samples, and y, a vector whose elements are the target values.  The goal is to model the data using an SVM or Support Vector Machine.  The program uses quadratic programming, which is a means of finding the maximum value of a convex (downward pointing) quadratic function given a set of linear inequalities and linear constraints.  For instance, in one dimension we might find the maximum of an upside-down parabola given the linear constraint x > 10.  However, the SVM is described by a set of equations that do not readily translate into a quadratic programming problem.  Therefore, we must utilize some heavy-duty math to translate the SVM problem into a \"dual problem\" which is amenable to quadratic programming.  We solve for a sequence of \"alpha\" values that correspond to the dual problem.  The SVM program then finds a w and b value, which can be used to make predictions via w.dot(x) + b > 0 vs. < 0, thereby classifying x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to minimize $(1/2)x^T P x + q^T x$ subject to  $G x \\le h$ and $A x = b$.\n",
    "\n",
    "So we want:\n",
    "\n",
    "x = alphas\n",
    "\n",
    "P = a matrix of kernel values (inner products between sample vectors) which are multiplied by 1 if the sample vectors are in the same class; otherwise by -1\n",
    "\n",
    "q = a vector of N 1's, whose length equals the number of samples.\n",
    "\n",
    "G = the identity matrix.\n",
    "\n",
    "h = a vertical vector of all 0's, whose length equals the number of samples.  These values of G and h together result in the constraint that all alphas are greater than zero.\n",
    "\n",
    "A = a vector of N y's (1s and -1s).  (Maybe it should say A^T x = b.)\n",
    "\n",
    "b = a scalar 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the prediction using a kernel function. The basic kernel function is x1.dot(x2), but that can also be squared; or a constant can be added and then it's squared, for instance. When the kernel is the square of a dot product, it represents an N^2 dimensional space where the values are the products of pairs of values in the x1 vector.  If x1 has dimension N, then the space of products of pairs of its values has dimension N^2, although the manifold of allowed values within that space still has dimension N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x1, x2):\n",
    "    return (x1.dot(x2) + 1000000.)**2 / 1000000000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel matrix finds the kernel value for each pair of rows of X; this becomes a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix(X):\n",
    "  return numpy.fromfunction(numpy.vectorize(lambda i, j: kernel(X[int(i)], X[int(j)])), (X.shape[0], X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some old data that could be input to the quadratic programming solver to find alpha. It represents a manifold of the from [x, x\\*\\*2] with x taking on values in [-2, -1, 0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the samples\n",
    "# X = numpy.array([[0.0, 0.0], [1.0, 1.0], [-1.0, 1.0], [2.0, 4.0], [-2.0, 4.0]])\n",
    "# y is the target value for each sample\n",
    "# y = numpy.array([-1.0, -1.0, 1.0, 1.0, 1.0])\n",
    "# ker = kernel_matrix(X)\n",
    "# We have to add an epsilon to make P be positive definite instead of just\n",
    "# nonnegative definite\n",
    "# P = numpy.multiply(ker, numpy.outer(y, y)) + numpy.eye(X.shape[0]) * 0.00001\n",
    "# q = -numpy.ones(X.shape[0])\n",
    "# G = -numpy.eye(X.shape[0])\n",
    "# h = numpy.zeros(X.shape[0])\n",
    "# A = y\n",
    " #b = numpy.array([0.0])\n",
    "# alpha = numpy.round(qpsolvers.solve_qp(P, q, G, h, A, b, solver = \"quadprog\"), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the value of b, the intercept in the expression w^T x + b that is used to define the decision boundary where we want to find the least magniture possible weight vector w such that $y^{(i)} (w^T x^{(i)} + b) \\ge 1$ for all i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_b(ker, y, alpha):\n",
    "  mx = numpy.NINF\n",
    "  mn = numpy.inf\n",
    "  for i in range(ker.shape[0]):\n",
    "    mx = max(mx, sum(alpha[j] * y[j] * ker[i, j] for j in range(ker.shape[0]))) if y[i] == -1 else mx\n",
    "    mn = min(mn, sum(alpha[j] * y[j] * ker[i, j] for j in range(ker.shape[0]))) if y[i] == 1 else mn\n",
    "  return -(mx + mn)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the formula for predicting using a kernel, from p. 13 of the Stanford CS 229 lecture by Andrew Ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_kernel(ker, X, y, alpha, x, b):\n",
    "  return numpy.round(sum(alpha[i] * y[i] * kernel(X[i],x) for i in range(ker.shape[0])) + b, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will apply the above functions to MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata(\"MNIST original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 9., 9., 9.])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM doesn't scale all that well with the number of samples.  Here, I will just use a random 6,000 training samples.  I can't use the _first_ 6,000 samples in MINST, because they all have a target value of zero.  Here, I am doing a one vs. all test, checking whether a given number is zero or not.  To do a more universal check, I would have to make ten different one vs. all tests or 45 one vs. one tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "nsamples = 6000\n",
    "\n",
    "indices = random.sample(range(1, 60000), nsamples)\n",
    "# X is the samples\n",
    "X = mnist.data[indices].astype(float)\n",
    "# y is the target value for each sample\n",
    "y = ((mnist.target[indices] == 0) * 2.0 - 1.0).astype(float)\n",
    "ker = kernel_matrix(X)\n",
    "# We have to add an epsilon to make P be positive definite instead of just\n",
    "# nonnegative definite\n",
    "P = numpy.multiply(ker, numpy.outer(y, y)) + numpy.eye(nsamples) * 1000000. #0.00001\n",
    "q = -numpy.ones(nsamples)\n",
    "G = -numpy.eye(nsamples)\n",
    "h = numpy.zeros(nsamples)\n",
    "A = y\n",
    "b = numpy.array([0.0])\n",
    "# alpha = qpsolvers.solve_qp(P, q, G, h, A, b, solver = \"CVXOPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27611.11541838,  6990.741288  ,  3629.5431785 , ...,\n",
       "         5006.28542573,  6369.37968017,  7970.67845112],\n",
       "       [ 6990.741288  , 45597.56624441,  2882.6470829 , ...,\n",
       "         9919.71559536,  6154.54229722,  8974.5779776 ],\n",
       "       [ 3629.5431785 ,  2882.6470829 , 11400.62394336, ...,\n",
       "         5853.3221645 ,  3295.15071502,  3753.03442563],\n",
       "       ...,\n",
       "       [ 5006.28542573,  9919.71559536,  5853.3221645 , ...,\n",
       "        46789.44415896, 13918.40602464, 20096.47310228],\n",
       "       [ 6369.37968017,  6154.54229722,  3295.15071502, ...,\n",
       "        13918.40602464, 54098.58457028, 14854.0097281 ],\n",
       "       [ 7970.67845112,  8974.5779776 ,  3753.03442563, ...,\n",
       "        20096.47310228, 14854.0097281 , 66783.85583532]])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = qpsolvers.solve_qp(P, q, G, h, A, b, solver = \"quadprog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.42169376e-25, 3.04549590e-07, 4.99499866e-08, ...,\n",
       "       7.16446049e-08, 1.10909317e-08, 1.91659718e-21])"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = compute_b(ker, y, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  1.144 True Actual: True\n",
      "Prediction:  0.895 True Actual: True\n",
      "Prediction:  0.972 True Actual: True\n",
      "Prediction:  2.078 True Actual: True\n",
      "Prediction:  1.206 True Actual: True\n",
      "Prediction:  0.533 True Actual: True\n",
      "Prediction:  0.71 True Actual: True\n",
      "Prediction:  1.835 True Actual: True\n",
      "Prediction:  0.859 True Actual: True\n",
      "Prediction:  0.322 True Actual: True\n",
      "Prediction:  -1.683 False Actual: False\n",
      "Prediction:  -1.014 False Actual: False\n",
      "Prediction:  -0.825 False Actual: False\n",
      "Prediction:  -0.54 False Actual: False\n",
      "Prediction:  -0.651 False Actual: False\n",
      "Prediction:  -0.733 False Actual: False\n",
      "Prediction:  -0.494 False Actual: False\n",
      "Prediction:  -0.639 False Actual: False\n",
      "Prediction:  -0.581 False Actual: False\n",
      "Prediction:  -0.596 False Actual: False\n"
     ]
    }
   ],
   "source": [
    "for n in list(range(60000, 60010)) + list(range(65000, 65010)):\n",
    "    inp = mnist.data[n].astype(float)\n",
    "    pred = pred_kernel(ker, X, y, alpha, inp, b)\n",
    "\n",
    "    print(\"Prediction: \", pred, bool((pred / numpy.abs(pred) + 1.0) / 2.0) if pred != 0.0 else True, \"Actual:\", mnist.target[n] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict all 10,000 \"test\" items in MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959 193 21 8827\n"
     ]
    }
   ],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "\n",
    "for n in list(range(60000, 70000)):\n",
    "    inp = mnist.data[n].astype(float)\n",
    "    pred = pred_kernel(ker, X, y, alpha, inp, b)\n",
    "    pred = bool((pred / numpy.abs(pred) + 1.0) / 2.0) if pred != 0.0 else True\n",
    "    actual = (mnist.target[n] == 0)\n",
    "    if pred and actual:\n",
    "        TP += 1\n",
    "    if pred and not actual:\n",
    "        FP +=1\n",
    "    if not pred and actual:\n",
    "        FN +=1\n",
    "    if not pred and not actual:\n",
    "        TN +=1\n",
    "\n",
    "print(TP, FP, FN, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9786"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(959 + 8827)/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
